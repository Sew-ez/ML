{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Dir:  /content\n",
      "Dataset Dir:  /content/Datasets/logoDetector\n",
      "Training network heads\n",
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /content/LogoDetectorModel/trainingLogs/sewez20240618T0707/mask_rcnn_sewez_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - ETA: 0s - batch: 2.0000 - size: 1.0000 - loss: 4.4239 - rpn_class_loss: 0.1531 - rpn_bbox_loss: 2.1008 - mrcnn_class_loss: 0.2912 - mrcnn_bbox_loss: 1.0820 - mrcnn_mask_loss: 0.7967   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 123s 26s/step - batch: 2.0000 - size: 1.0000 - loss: 4.4239 - rpn_class_loss: 0.1531 - rpn_bbox_loss: 2.1008 - mrcnn_class_loss: 0.2912 - mrcnn_bbox_loss: 1.0820 - mrcnn_mask_loss: 0.7967 - val_loss: 2.9881 - val_rpn_class_loss: 0.1696 - val_rpn_bbox_loss: 1.1075 - val_mrcnn_class_loss: 0.4224 - val_mrcnn_bbox_loss: 0.8627 - val_mrcnn_mask_loss: 0.4260\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 95s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 2.4549 - rpn_class_loss: 0.0262 - rpn_bbox_loss: 0.3922 - mrcnn_class_loss: 0.5205 - mrcnn_bbox_loss: 0.8144 - mrcnn_mask_loss: 0.7017 - val_loss: 2.4629 - val_rpn_class_loss: 0.1131 - val_rpn_bbox_loss: 0.8953 - val_mrcnn_class_loss: 0.3673 - val_mrcnn_bbox_loss: 0.7518 - val_mrcnn_mask_loss: 0.3354\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 95s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 2.0671 - rpn_class_loss: 0.0182 - rpn_bbox_loss: 0.4313 - mrcnn_class_loss: 0.3187 - mrcnn_bbox_loss: 0.8954 - mrcnn_mask_loss: 0.4036 - val_loss: 2.1501 - val_rpn_class_loss: 0.0879 - val_rpn_bbox_loss: 0.8260 - val_mrcnn_class_loss: 0.2108 - val_mrcnn_bbox_loss: 0.7315 - val_mrcnn_mask_loss: 0.2939\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 92s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 3.1304 - rpn_class_loss: 0.0555 - rpn_bbox_loss: 1.3531 - mrcnn_class_loss: 0.3216 - mrcnn_bbox_loss: 0.9493 - mrcnn_mask_loss: 0.4508 - val_loss: 2.3305 - val_rpn_class_loss: 0.0819 - val_rpn_bbox_loss: 0.9116 - val_mrcnn_class_loss: 0.1797 - val_mrcnn_bbox_loss: 0.8950 - val_mrcnn_mask_loss: 0.2622\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 95s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 2.1420 - rpn_class_loss: 0.0435 - rpn_bbox_loss: 0.8865 - mrcnn_class_loss: 0.1113 - mrcnn_bbox_loss: 0.7470 - mrcnn_mask_loss: 0.3537 - val_loss: 2.3622 - val_rpn_class_loss: 0.1820 - val_rpn_bbox_loss: 1.1093 - val_mrcnn_class_loss: 0.1877 - val_mrcnn_bbox_loss: 0.6157 - val_mrcnn_mask_loss: 0.2675\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 89s 19s/step - batch: 2.0000 - size: 1.0000 - loss: 1.6692 - rpn_class_loss: 0.0166 - rpn_bbox_loss: 0.6294 - mrcnn_class_loss: 0.1521 - mrcnn_bbox_loss: 0.5240 - mrcnn_mask_loss: 0.3472 - val_loss: 2.3777 - val_rpn_class_loss: 0.0945 - val_rpn_bbox_loss: 1.2451 - val_mrcnn_class_loss: 0.1668 - val_mrcnn_bbox_loss: 0.5990 - val_mrcnn_mask_loss: 0.2723\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 94s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 1.7274 - rpn_class_loss: 0.0320 - rpn_bbox_loss: 0.5862 - mrcnn_class_loss: 0.1968 - mrcnn_bbox_loss: 0.5161 - mrcnn_mask_loss: 0.3964 - val_loss: 1.9910 - val_rpn_class_loss: 0.0766 - val_rpn_bbox_loss: 0.8427 - val_mrcnn_class_loss: 0.2460 - val_mrcnn_bbox_loss: 0.5534 - val_mrcnn_mask_loss: 0.2723\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 92s 19s/step - batch: 2.0000 - size: 1.0000 - loss: 2.1165 - rpn_class_loss: 0.0261 - rpn_bbox_loss: 0.9468 - mrcnn_class_loss: 0.2390 - mrcnn_bbox_loss: 0.5362 - mrcnn_mask_loss: 0.3684 - val_loss: 1.6428 - val_rpn_class_loss: 0.0406 - val_rpn_bbox_loss: 0.5823 - val_mrcnn_class_loss: 0.1973 - val_mrcnn_bbox_loss: 0.5762 - val_mrcnn_mask_loss: 0.2464\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 95s 19s/step - batch: 2.0000 - size: 1.0000 - loss: 1.9604 - rpn_class_loss: 0.0391 - rpn_bbox_loss: 0.8674 - mrcnn_class_loss: 0.1934 - mrcnn_bbox_loss: 0.5190 - mrcnn_mask_loss: 0.3415 - val_loss: 1.7530 - val_rpn_class_loss: 0.0961 - val_rpn_bbox_loss: 0.6941 - val_mrcnn_class_loss: 0.2014 - val_mrcnn_bbox_loss: 0.5251 - val_mrcnn_mask_loss: 0.2363\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 97s 21s/step - batch: 2.0000 - size: 1.0000 - loss: 1.9707 - rpn_class_loss: 0.0261 - rpn_bbox_loss: 0.9288 - mrcnn_class_loss: 0.1357 - mrcnn_bbox_loss: 0.5421 - mrcnn_mask_loss: 0.3381 - val_loss: 1.7670 - val_rpn_class_loss: 0.0961 - val_rpn_bbox_loss: 0.6401 - val_mrcnn_class_loss: 0.2062 - val_mrcnn_bbox_loss: 0.5457 - val_mrcnn_mask_loss: 0.2788\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 89s 18s/step - batch: 2.0000 - size: 1.0000 - loss: 1.7231 - rpn_class_loss: 0.0305 - rpn_bbox_loss: 0.7021 - mrcnn_class_loss: 0.1640 - mrcnn_bbox_loss: 0.5363 - mrcnn_mask_loss: 0.2903 - val_loss: 1.4086 - val_rpn_class_loss: 0.0136 - val_rpn_bbox_loss: 0.3533 - val_mrcnn_class_loss: 0.2315 - val_mrcnn_bbox_loss: 0.5597 - val_mrcnn_mask_loss: 0.2506\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 96s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 2.3330 - rpn_class_loss: 0.0752 - rpn_bbox_loss: 1.0996 - mrcnn_class_loss: 0.1632 - mrcnn_bbox_loss: 0.5517 - mrcnn_mask_loss: 0.4433 - val_loss: 1.5554 - val_rpn_class_loss: 0.0467 - val_rpn_bbox_loss: 0.3986 - val_mrcnn_class_loss: 0.2621 - val_mrcnn_bbox_loss: 0.5881 - val_mrcnn_mask_loss: 0.2598\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 92s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 1.6589 - rpn_class_loss: 0.0449 - rpn_bbox_loss: 0.5838 - mrcnn_class_loss: 0.2229 - mrcnn_bbox_loss: 0.4754 - mrcnn_mask_loss: 0.3319 - val_loss: 1.5896 - val_rpn_class_loss: 0.0252 - val_rpn_bbox_loss: 0.3775 - val_mrcnn_class_loss: 0.2112 - val_mrcnn_bbox_loss: 0.6215 - val_mrcnn_mask_loss: 0.3542\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 99s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 2.0812 - rpn_class_loss: 0.0360 - rpn_bbox_loss: 0.7425 - mrcnn_class_loss: 0.3912 - mrcnn_bbox_loss: 0.5318 - mrcnn_mask_loss: 0.3795 - val_loss: 1.6807 - val_rpn_class_loss: 0.1045 - val_rpn_bbox_loss: 0.5658 - val_mrcnn_class_loss: 0.1763 - val_mrcnn_bbox_loss: 0.5996 - val_mrcnn_mask_loss: 0.2345\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 100s 21s/step - batch: 2.0000 - size: 1.0000 - loss: 1.9254 - rpn_class_loss: 0.0458 - rpn_bbox_loss: 0.5651 - mrcnn_class_loss: 0.3339 - mrcnn_bbox_loss: 0.6084 - mrcnn_mask_loss: 0.3723 - val_loss: 1.5006 - val_rpn_class_loss: 0.0347 - val_rpn_bbox_loss: 0.3899 - val_mrcnn_class_loss: 0.2092 - val_mrcnn_bbox_loss: 0.6265 - val_mrcnn_mask_loss: 0.2404\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 100s 21s/step - batch: 2.0000 - size: 1.0000 - loss: 1.7132 - rpn_class_loss: 0.0544 - rpn_bbox_loss: 0.5764 - mrcnn_class_loss: 0.1720 - mrcnn_bbox_loss: 0.4921 - mrcnn_mask_loss: 0.4183 - val_loss: 1.7360 - val_rpn_class_loss: 0.1081 - val_rpn_bbox_loss: 0.5777 - val_mrcnn_class_loss: 0.1914 - val_mrcnn_bbox_loss: 0.5267 - val_mrcnn_mask_loss: 0.3321\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 94s 19s/step - batch: 2.0000 - size: 1.0000 - loss: 1.6113 - rpn_class_loss: 0.0216 - rpn_bbox_loss: 0.6169 - mrcnn_class_loss: 0.1893 - mrcnn_bbox_loss: 0.4208 - mrcnn_mask_loss: 0.3629 - val_loss: 1.2154 - val_rpn_class_loss: 0.0294 - val_rpn_bbox_loss: 0.3647 - val_mrcnn_class_loss: 0.1836 - val_mrcnn_bbox_loss: 0.4248 - val_mrcnn_mask_loss: 0.2130\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 96s 20s/step - batch: 2.0000 - size: 1.0000 - loss: 1.5217 - rpn_class_loss: 0.0300 - rpn_bbox_loss: 0.4868 - mrcnn_class_loss: 0.2571 - mrcnn_bbox_loss: 0.4370 - mrcnn_mask_loss: 0.3108 - val_loss: 1.7696 - val_rpn_class_loss: 0.0978 - val_rpn_bbox_loss: 0.5443 - val_mrcnn_class_loss: 0.2709 - val_mrcnn_bbox_loss: 0.5739 - val_mrcnn_mask_loss: 0.2827\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 99s 21s/step - batch: 2.0000 - size: 1.0000 - loss: 1.3582 - rpn_class_loss: 0.0208 - rpn_bbox_loss: 0.6403 - mrcnn_class_loss: 0.1146 - mrcnn_bbox_loss: 0.3349 - mrcnn_mask_loss: 0.2477 - val_loss: 1.3905 - val_rpn_class_loss: 0.0329 - val_rpn_bbox_loss: 0.3961 - val_mrcnn_class_loss: 0.2214 - val_mrcnn_bbox_loss: 0.5184 - val_mrcnn_mask_loss: 0.2216\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 104s 22s/step - batch: 2.0000 - size: 1.0000 - loss: 1.5148 - rpn_class_loss: 0.0294 - rpn_bbox_loss: 0.4372 - mrcnn_class_loss: 0.2228 - mrcnn_bbox_loss: 0.5293 - mrcnn_mask_loss: 0.2962 - val_loss: 1.5467 - val_rpn_class_loss: 0.0463 - val_rpn_bbox_loss: 0.5419 - val_mrcnn_class_loss: 0.2108 - val_mrcnn_bbox_loss: 0.4908 - val_mrcnn_mask_loss: 0.2568\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 88s 18s/step - batch: 2.0000 - size: 1.0000 - loss: 1.6065 - rpn_class_loss: 0.0390 - rpn_bbox_loss: 0.5964 - mrcnn_class_loss: 0.1536 - mrcnn_bbox_loss: 0.4801 - mrcnn_mask_loss: 0.3374 - val_loss: 1.2801 - val_rpn_class_loss: 0.0088 - val_rpn_bbox_loss: 0.4028 - val_mrcnn_class_loss: 0.1752 - val_mrcnn_bbox_loss: 0.4650 - val_mrcnn_mask_loss: 0.2283\n",
      "Epoch 22/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-61ca0d47a5a8>\u001b[0m in \u001b[0;36m<cell line: 234>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m             \"mrcnn_bbox\", \"mrcnn_mask\"])\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-61ca0d47a5a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m#             epochs=30,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m#             layers='heads')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     model.train(dataset_train, dataset_val,\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2371\u001b[0m             \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m         self.keras_model.fit(\n\u001b[0m\u001b[1;32m   2374\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         return func.fit(\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         )\n\u001b[0;32m--> 647\u001b[0;31m         return fit_generator(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4607\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4609\u001b[0m         output_structure = tf.nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1506\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import imgaug\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "np.bool = np.bool_\n",
    "\n",
    "# Root directory of the project\n",
    "# ROOT_DIR = os.path.abspath(\"../\")\n",
    "ROOT_DIR = os.getcwd()\n",
    "ROOT_DIR = os.path.normpath(ROOT_DIR)\n",
    "print(\"Working Dir: \", ROOT_DIR)\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, 'Datasets/logoDetector/')\n",
    "DATASET_DIR = os.path.normpath(DATASET_DIR)\n",
    "print(\"Dataset Dir: \", DATASET_DIR)\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"LogoDetectorModel/preTrainedModel/mask_rcnn_coco.h5\")\n",
    "\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"LogoDetectorModel/trainingLogs\")\n",
    "\n",
    "############################################################\n",
    "#  Configurations\n",
    "############################################################\n",
    "\n",
    "class TrainingConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"sewez\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # Background + sewez_tshirt, sewez_tshirt_logo, sewez_tshirt_brand\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 5\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "\n",
    "    # Training Learning Rate\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "\n",
    "class TrainingDataset(utils.Dataset):\n",
    "\n",
    "    def load_training_dataset(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Balloon dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"sewez\", 1, \"sewez_tshirt\")\n",
    "        self.add_class(\"sewez\", 2, \"sewez_tshirt_logo\")\n",
    "        self.add_class(\"sewez\", 3, \"sewez_tshirt_brand\")\n",
    "\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        annotations = json.load(open(os.path.join(dataset_dir, \"annotation.json\")))\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\n",
    "            # the outline of each object instance. These are stores in the\n",
    "            # shape_attributes (see json format above)\n",
    "            # The if condition is needed to support VIA versions 1.x and 2.x.\n",
    "            if type(a['regions']) is dict:\n",
    "                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n",
    "            else:\n",
    "                polygons = [r['shape_attributes'] for r in a['regions']] \n",
    "\n",
    "            # iterate names\n",
    "            objects = [s['region_attributes']['names'] for s in a['regions']]\n",
    "            name_dict = {\"sewez_tshirt\": 1, \"sewez_tshirt_logo\": 2, \"sewez_tshirt_brand\": 3}\n",
    "            num_ids = [name_dict[a] for a in objects]\n",
    "\n",
    "            # load_mask() needs the image size to convert polygons to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"sewez\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a Dog-Cat dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"sewez\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] != \"sewez\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        num_ids = info['num_ids']\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        # Map class names to class IDs.\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"object\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = TrainingDataset()\n",
    "    dataset_train.load_training_dataset(DATASET_DIR, \"train\")\n",
    "    dataset_train.prepare()\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = TrainingDataset()\n",
    "    dataset_val.load_training_dataset(DATASET_DIR, \"val\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # *** This training schedule is an example. Update to your needs ***\n",
    "    # Since we're using a very small dataset, and starting from\n",
    "    # COCO trained weights, we don't need to train too long. Also,\n",
    "    # no need to train all layers, just the heads should do it.\n",
    "    print(\"Training network heads\")\n",
    "    # model.train(dataset_train, dataset_val,\n",
    "    #             learning_rate=config.LEARNING_RATE,\n",
    "    #             epochs=30,\n",
    "    #             layers='heads')\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=300,\n",
    "                layers='heads', #layers='all', \n",
    "                augmentation = imgaug.augmenters.Sequential([ \n",
    "                imgaug.augmenters.Fliplr(1), \n",
    "                imgaug.augmenters.Flipud(1), \n",
    "                imgaug.augmenters.Affine(rotate=(-45, 45)), \n",
    "                imgaug.augmenters.Affine(rotate=(-90, 90)), \n",
    "                imgaug.augmenters.Affine(scale=(0.5, 1.5)),\n",
    "                imgaug.augmenters.Crop(px=(0, 10)),\n",
    "                imgaug.augmenters.Grayscale(alpha=(0.0, 1.0)),\n",
    "                imgaug.augmenters.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n",
    "                imgaug.augmenters.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                imgaug.augmenters.Invert(0.05, per_channel=True), # invert color channels\n",
    "                imgaug.augmenters.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                ]\n",
    "                ))\n",
    "\n",
    "############################################################\n",
    "#  Training\n",
    "############################################################\n",
    "\n",
    "config = TrainingConfig()\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                                  model_dir=DEFAULT_LOGS_DIR)\n",
    "\n",
    "weights_path = COCO_WEIGHTS_PATH\n",
    "        # Download weights file\n",
    "if not os.path.exists(weights_path):\n",
    "  utils.download_trained_weights(weights_path)\n",
    "\n",
    "model.load_weights(weights_path, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
